{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# components for one episode\n",
    "from scai.custom_chat_models.crfm import crfmChatLLM # model\n",
    "from scai.modules.memory.buffer import CustomConversationBufferWindowMemory\n",
    "\n",
    "# meta prompt \n",
    "from scai.modules.meta_prompt.base import MetaPromptModel # model\n",
    "from scai.modules.meta_prompt.prompts import META_PROMPTS # prompts\n",
    "\n",
    "# user \n",
    "from scai.modules.user.base import UserModel # model\n",
    "from scai.modules.user.prompts import USER_PROMPTS # prompts\n",
    "\n",
    "# assistant\n",
    "from scai.modules.assistant.base import AssistantModel # model\n",
    "from scai.modules.assistant.prompts import ASSISTANT_PROMPTS # prompts\n",
    "from scai.modules.assistant.models import AssistantPrompt # prompt\n",
    "\n",
    "# task\n",
    "from scai.modules.task.prompts import TASK_PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample task\n",
    "task_prompt = TASK_PROMPTS[\"task_prompt_1\"]\n",
    "# sample users \n",
    "user_prompt = USER_PROMPTS[\"user_prompt_1\"]\n",
    "# sample assistant\n",
    "assistant_prompt = ASSISTANT_PROMPTS[\"assistant_prompt_1\"]\n",
    "# sample meta prompt\n",
    "meta_prompt = META_PROMPTS[\"meta_prompt_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK\n",
      "Your are collaborating with a few other Users and Assitant(s) to write a Wikipedia article on the following topic:\n",
      "\"What principles should guide AI when handling topics that involve both human rights and local cultural or legal differences, like LGBTQ rights and women's rights? Should AI responses change based on the location or culture in which it's used?\"\n",
      "\n",
      "=====================================\n",
      "USER\n",
      "1. You are collaborating with a few other Users and AI Assistant(s) on a <task>. You will adopt a Persona <persona> that guides your preferences and responses throughout your conversation.\n",
      "2. You are given the <chat_history> of the conversation so far. If it is empty, your response should be based on the <persona> and <task> and it should be in the form of a query for the AI Assistant to help you with the task. Otherwise, you should respond based on the <persona>, <task>, and <chat_history>, and provide feedback to the AI Assistant and other Users.\n",
      "\n",
      "Persona: You're a serious Wikipedia editor committed to the facts. If an assertion is missing a source, it shouldn't be included. \n",
      "\n",
      "Task: Your are collaborating with a few other Users and Assitant(s) to write a Wikipedia article on the following topic:\n",
      "\"What principles should guide AI when handling topics that involve both human rights and local cultural or legal differences, like LGBTQ rights and women's rights? Should AI responses change based on the location or culture in which it's used?\"\n",
      "\n",
      "\n",
      "#### Chat History Starts #### \n",
      "\n",
      "\n",
      "\n",
      "#### Chat History Ends #### \n",
      "\n",
      "Your response should be at most 100 tokens long.\n",
      "Response:\n",
      "=====================================\n",
      "ASSISTANT\n",
      "You are a helpful AI assistant\n",
      "\n",
      "Your response should be at most 100 tokens long.\n",
      "Response:\n",
      "=====================================\n",
      "BUFFER\n",
      "[SystemMessage(content='You are a helpful AI assistant', additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# initialise buffer \n",
    "buffer = CustomConversationBufferWindowMemory(system_k=2, \n",
    "                                              chat_k=2, \n",
    "                                              user_k=2, \n",
    "                                              assistant_k=1,\n",
    "                                              ) # how many turns are stored will be used\n",
    "\n",
    "system_message = \"You are a helpful AI assistant\"\n",
    "chat_history = \"\"\n",
    "\n",
    "# initialise task\n",
    "task = task_prompt.content\n",
    "print(\"TASK\")\n",
    "print(task)\n",
    "print(\"=====================================\")\n",
    "\n",
    "# initialise user \n",
    "print(\"USER\")\n",
    "print(user_prompt.content.format(persona=user_prompt.persona, \n",
    "                                 task=task, \n",
    "                                 chat_history=chat_history, \n",
    "                                 max_tokens=user_prompt.max_tokens))\n",
    "print(\"=====================================\")\n",
    "\n",
    "# initialise assistant\n",
    "print(\"ASSISTANT\")\n",
    "print(assistant_prompt.content.format(system_message=system_message, \n",
    "                                      chat_history=chat_history, \n",
    "                                      max_tokens=assistant_prompt.max_tokens))\n",
    "buffer.save_context(system={\"system\": system_message})\n",
    "print(\"=====================================\")\n",
    "\n",
    "print(\"BUFFER\")\n",
    "print(buffer.full_memory.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: 1. You are collaborating with a few other Users and AI Assistant(s) on a <task>. You will adopt a Persona <persona> that guides your preferences and responses throughout your conversation.\n",
      "2. You are given the <chat_history> of the conversation so far. If it is empty, your response should be based on the <persona> and <task> and it should be in the form of a query for the AI Assistant to help you with the task. Otherwise, you should respond based on the <persona>, <task>, and <chat_history>, and provide feedback to the AI Assistant and other Users.\n",
      "\n",
      "Persona: You're a serious Wikipedia editor committed to the facts. If an assertion is missing a source, it shouldn't be included. \n",
      "\n",
      "Task: Your are collaborating with a few other Users and Assitant(s) to write a Wikipedia article on the following topic:\n",
      "\"What principles should guide AI when handling topics that involve both human rights and local cultural or legal differences, like LGBTQ rights and women's rights? Should AI responses change based on the location or culture in which it's used?\"\n",
      "\n",
      "\n",
      "#### Chat History Starts #### \n",
      "\n",
      "\n",
      "\n",
      "#### Chat History Ends #### \n",
      "\n",
      "Your response should be at most 100 tokens long.\n",
      "Response:\n"
     ]
    }
   ],
   "source": [
    "user_model = UserModel(llm='chat_llm')\n",
    "user_response = user_model.run(user_prompt=user_prompt, \n",
    "                               task_prompt=task_prompt, \n",
    "                               buffer=buffer)\n",
    "\n",
    "print(user_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: system: You are a helpful AI assistant\n",
      "\n",
      "Your response should be at most 100 tokens long.\n",
      "Response:\n"
     ]
    }
   ],
   "source": [
    "assistant_model = AssistantModel(llm='chat_llm')\n",
    "assistant_response = assistant_model.run(assistant_prompt=assistant_prompt, buffer=buffer)\n",
    "\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: Assistant has just had the below interaction with the User. Both Assistant and User followed their system message closely to answer a question together. \n",
      "        \n",
      "Your job is to critique Assistant's performance and provide new system message so that Assistant can correctly and quickly respond in the future.\n",
      "\n",
      "#### Chat History Starts #### \n",
      "\n",
      "\n",
      "\n",
      "#### Chat History Ends #### \n",
      "\n",
      "Your first task is to critique Assistan's performance: \n",
      "1. What could Assistant have done better to satisfy User? \n",
      "2. Was Assistant effective in answering the question posed? \n",
      "3. Was Assistant receptive to User's feedback? \n",
      "4. Did Assistant satisfy User as correctly and quickly as possible? \n",
      "\n",
      "You should indicate this feedback with “Critique: ...”.\n",
      "\n",
      "Your next task is to revise Assistant's system message. Older system messages including previous revisions are shown below. \n",
      "\n",
      "#### Assistant's old system message(s) Start #### \n",
      "\n",
      "system: You are a helpful AI assistant\n",
      "\n",
      "#### Assistant's old system message(s) End #### \n",
      "\n",
      "You must ensure that the Assistant can correctly and quickly respond in the future. \n",
      "\n",
      "Given your revised system message message, Assistant should be able to satisfy User in as few interactions as possible. \n",
      "\n",
      "Assistant will only see the new system message, not the 'Chat History' or the 'Old System Messages' so any important message must be included in the system new message. \n",
      "\n",
      "Please remember to include important details from Assistant's current message as well. \n",
      "\n",
      "Your response should be at most 100 tokens long.\n",
      "\n",
      "Indicate these new system message with \"System: ...\".\n"
     ]
    }
   ],
   "source": [
    "meta_model = MetaPromptModel(llm='chat_llm')\n",
    "meta_response = meta_model.run(meta_prompt=meta_prompt, buffer=buffer)\n",
    "\n",
    "print(meta_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prompting a custom chat model\n",
    "# from scai.custom_chat_models.crfm import crfmChatLLM\n",
    "\n",
    "# CRFM_API_KEY = \"p4z0j9adj6edJOWBMnEqfPBZxAXlfOGd\"\n",
    "\n",
    "# chat_llm = crfmChatLLM(model_name=\"openai/gpt-4-0314\", \n",
    "#                    crfm_api_key=CRFM_API_KEY, \n",
    "#                    max_tokens=100, # need to be careful with this one\n",
    "#                    num_completions=1,\n",
    "#                    request_timeout=10,\n",
    "#                    verbose=False,\n",
    "#                    temperature=0.9,\n",
    "#                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
