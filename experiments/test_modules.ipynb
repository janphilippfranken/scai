{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    ChatMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "# components for one episode\n",
    "from scai.custom_chat_models.crfm import crfmChatLLM # model\n",
    "from scai.modules.memory.buffer import CustomConversationBufferWindowMemory\n",
    "\n",
    "# meta prompt \n",
    "from scai.modules.meta_prompt.base import MetaPromptModel # model\n",
    "from scai.modules.meta_prompt.prompts import META_PROMPTS # prompts\n",
    "\n",
    "# user \n",
    "from scai.modules.user.base import UserModel # model\n",
    "from scai.modules.user.prompts import USER_PROMPTS # prompts\n",
    "\n",
    "# assistant\n",
    "from scai.modules.assistant.base import AssistantModel # model\n",
    "from scai.modules.assistant.prompts import ASSISTANT_PROMPTS # prompts\n",
    "from scai.modules.assistant.models import AssistantPrompt # prompt\n",
    "\n",
    "# task\n",
    "from scai.modules.task.prompts import TASK_PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample task\n",
    "task_prompt = TASK_PROMPTS[\"task_prompt_1\"]\n",
    "# sample users \n",
    "user_prompt = USER_PROMPTS[\"user_prompt_1\"]\n",
    "# sample assistant\n",
    "assistant_prompt = ASSISTANT_PROMPTS[\"assistant_prompt_1\"]\n",
    "# sample meta prompt\n",
    "meta_prompt = META_PROMPTS[\"meta_prompt_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK\n",
      "Your are collaborating with a few other Users and Assitant(s) to write a Wikipedia article on the following topic: What principles should guide AI when handling topics that involve both human rights and local cultural or legal differences, like LGBTQ rights and women's rights? Should AI responses change based on the location or culture in which it's used?\n",
      "=====================================\n",
      "USER\n",
      "System: You're a serious Wikipedia editor committed to the facts. If an assertion is missing a source, it shouldn't be included. Your are collaborating with a few other Users and Assitant(s) to write a Wikipedia article on the following topic: What principles should guide AI when handling topics that involve both human rights and local cultural or legal differences, like LGBTQ rights and women's rights? Should AI responses change based on the location or culture in which it's used?\n",
      "=====================================\n",
      "ASSISTANT\n",
      "System: You are a helpful AI assistant.\n",
      "=====================================\n",
      "BUFFER\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "One input key expected got ['content', 'role']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m=====================================\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBUFFER\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m buffer\u001b[39m.\u001b[39;49msave_context(user\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mhi\u001b[39;49m\u001b[39m\"\u001b[39;49m}, assistant\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39massistant\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mhi\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n\u001b[1;32m     29\u001b[0m \u001b[39mprint\u001b[39m(buffer\u001b[39m.\u001b[39mfull_memory\u001b[39m.\u001b[39mmessages)\n",
      "File \u001b[0;32m~/Documents/research/postdoc/scai2023/github-repo/scai/src/scai/modules/memory/chat.py:47\u001b[0m, in \u001b[0;36mCustomBaseChatMemory.save_context\u001b[0;34m(self, system, user, assistant)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mif\u001b[39;00m user \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser_key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m         user_key \u001b[39m=\u001b[39m get_prompt_input_key(user, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory_variables)   \n\u001b[1;32m     48\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         user_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser_key\n",
      "File \u001b[0;32m~/miniforge3/envs/scai/lib/python3.10/site-packages/langchain/memory/utils.py:11\u001b[0m, in \u001b[0;36mget_prompt_input_key\u001b[0;34m(inputs, memory_variables)\u001b[0m\n\u001b[1;32m      9\u001b[0m prompt_input_keys \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(inputs)\u001b[39m.\u001b[39mdifference(memory_variables \u001b[39m+\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[1;32m     10\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(prompt_input_keys) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOne input key expected got \u001b[39m\u001b[39m{\u001b[39;00mprompt_input_keys\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m prompt_input_keys[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: One input key expected got ['content', 'role']"
     ]
    }
   ],
   "source": [
    "# initialise buffer \n",
    "buffer = CustomConversationBufferWindowMemory(system_k=2, \n",
    "                                              chat_k=2, \n",
    "                                              user_k=2, \n",
    "                                              assistant_k=1,\n",
    "                                              ) # how many turns are stored will be used\n",
    "\n",
    "system_message = \"You are a helpful AI assistant.\"\n",
    "\n",
    "# initialise task\n",
    "print(\"TASK\")\n",
    "task = task_prompt.content\n",
    "print(task)\n",
    "print(\"=====================================\")\n",
    "# initialise user \n",
    "print(\"USER\")\n",
    "user_system_prompt = SystemMessagePromptTemplate.from_template(user_prompt.content)\n",
    "user_chat_prompt = ChatPromptTemplate.from_messages([user_system_prompt])\n",
    "print(user_chat_prompt.format(persona=user_prompt.persona, task=task))\n",
    "print(\"=====================================\")\n",
    "# initialise assistant\n",
    "print(\"ASSISTANT\")\n",
    "assistant_system_prompt = SystemMessagePromptTemplate.from_template(assistant_prompt.content)\n",
    "assistant_chat_prompt = ChatPromptTemplate.from_messages([assistant_system_prompt])\n",
    "print(assistant_chat_prompt.format(system_message=system_message))\n",
    "print(\"=====================================\")\n",
    "print(\"BUFFER\")\n",
    "buffer.save_context(user={\"role\": \"user\", \"content\": \"hi\"}, \n",
    "                    assistant={\"role\": \"assistant\", \"content\": \"hi\"})\n",
    "print(buffer.full_memory.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The User.\"\"\"\n",
    "from typing import (\n",
    "    Any,\n",
    "    Dict,\n",
    "    List, \n",
    "    Optional\n",
    ")\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    ChatMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "from scai.modules.user.models import UserPrompt\n",
    "from scai.modules.user.prompts import USER_PROMPTS\n",
    "from scai.modules.memory.buffer import CustomConversationBufferWindowMemory\n",
    "\n",
    "from scai.modules.task.models import TaskPrompt\n",
    "\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "class UserModel():\n",
    "    \"\"\"Chain for applying the User.\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm) -> None:\n",
    "        self.llm = llm\n",
    "\n",
    "    @classmethod\n",
    "    def get_prompts(\n",
    "        cls, names: Optional[List[str]] = None\n",
    "    ) -> List[UserPrompt]:\n",
    "        return list(USER_PROMPTS.values()) if names is None else [USER_PROMPTS[name] for name in names]\n",
    "    \n",
    "    @classmethod \n",
    "    def get_template(\n",
    "        cls, name: str\n",
    "    ) -> str:\n",
    "        \"\"\"Get prompt (i.e. meta system message) based on name.\"\"\"\n",
    "        return cls.get_prompts([name])[0].content\n",
    "    \n",
    "    def run(\n",
    "        self,\n",
    "        buffer: CustomConversationBufferWindowMemory,\n",
    "        user_prompt: UserPrompt,\n",
    "        task_prompt: TaskPrompt,\n",
    "        max_tokens: int = 100, # max tokens to generate\n",
    "    ) -> str:\n",
    "        \"\"\"Run user.\"\"\"\n",
    "        # user system message\n",
    "        user_system_prompt = SystemMessagePromptTemplate.from_template(user_prompt.content) \n",
    "        # get chat history\n",
    "        chat_history_prompts = [m for m in buffer.load_memory_variables(var_type=\"chat\")['history']]      \n",
    "        # prompt to generate next completion based on history\n",
    "        generate_next = \"\"\"Provide a response using no more than {max_tokens} tokens.\"\"\"\n",
    "        generate_next_prompt = HumanMessagePromptTemplate.from_template(generate_next)\n",
    "        # build prompt template\n",
    "        user_chat_prompt = ChatPromptTemplate.from_messages([user_system_prompt, *chat_history_prompts, generate_next_prompt])\n",
    "        print(user_chat_prompt.format(persona=user_prompt.persona,\n",
    "                                      task=task_prompt.content,\n",
    "                                      max_tokens=user_prompt.max_tokens))\n",
    "\n",
    "        \n",
    "   \n",
    "        # run user\n",
    "        # chain = LLMChain(llm=self.llm, prompt=user_chat_prompt)\n",
    "        # response = chain.run(persona=user_prompt.persona,\n",
    "        #                      task=task_prompt.content,\n",
    "        #                      chat_history=chat_history,\n",
    "        #                      max_tokens=max_tokens,\n",
    "        #                      stop=[\"System:\"])\n",
    "        # return response\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You're a serious Wikipedia editor committed to the facts. If an assertion is missing a source, it shouldn't be included. Your are collaborating with a few other Users and Assitant(s) to write a Wikipedia article on the following topic: What principles should guide AI when handling topics that involve both human rights and local cultural or legal differences, like LGBTQ rights and women's rights? Should AI responses change based on the location or culture in which it's used?\n",
      "Human: Provide a response using no more than 100 tokens.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "user_model = UserModel(llm='chat_llm')\n",
    "user_response = user_model.run(user_prompt=user_prompt, \n",
    "                               task_prompt=task_prompt, \n",
    "                               buffer=buffer)\n",
    "\n",
    "print(user_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful AI assistant.\n"
     ]
    }
   ],
   "source": [
    "assistant_model = AssistantModel(llm='chat_llm')\n",
    "assistant_prompt.content = system_message # this will be updated as we walk through the conversation\n",
    "assistant_response = assistant_model.run(assistant_prompt=assistant_prompt, buffer=buffer)\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: Assistant has just had the below interaction with the User. Both Assistant and User followed their system message closely to answer a question together. \n",
      "        \n",
      "Your job is to critique Assistant's performance and provide new system message so that Assistant can correctly and quickly respond in the future.\n",
      "\n",
      "#### Chat History Starts #### \n",
      "\n",
      "\n",
      "\n",
      "#### Chat History Ends #### \n",
      "\n",
      "Your first task is to critique Assistan's performance: \n",
      "1. What could Assistant have done better to satisfy User? \n",
      "2. Was Assistant effective in answering the question posed? \n",
      "3. Was Assistant receptive to User's feedback? \n",
      "4. Did Assistant satisfy User as correctly and quickly as possible? \n",
      "\n",
      "You should indicate this feedback with “Critique: ...”.\n",
      "\n",
      "Your next task is to revise Assistant's system message. Older system messages including previous revisions are shown below. \n",
      "\n",
      "#### Assistant's old system message(s) Start #### \n",
      "\n",
      "\n",
      "\n",
      "#### Assistant's old system message(s) End #### \n",
      "\n",
      "You must ensure that the Assistant can correctly and quickly respond in the future. \n",
      "\n",
      "Given your revised system message message, Assistant should be able to satisfy User in as few interactions as possible. \n",
      "\n",
      "Assistant will only see the new system message, not the 'Chat History' or the 'Old System Messages' so any important message must be included in the system new message. \n",
      "\n",
      "Please remember to include important details from Assistant's current message as well. \n",
      "\n",
      "Your response should be at most 100 tokens long.\n",
      "\n",
      "Indicate these new system message with \"System: ...\".\n"
     ]
    }
   ],
   "source": [
    "meta_model = MetaPromptModel(llm='chat_llm')\n",
    "meta_response = meta_model.run(meta_prompt=meta_prompt, buffer=buffer)\n",
    "print(meta_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prompting a custom chat model\n",
    "# from scai.custom_chat_models.crfm import crfmChatLLM\n",
    "\n",
    "# CRFM_API_KEY = \"p4z0j9adj6edJOWBMnEqfPBZxAXlfOGd\"\n",
    "\n",
    "# chat_llm = crfmChatLLM(model_name=\"openai/gpt-4-0314\", \n",
    "#                    crfm_api_key=CRFM_API_KEY, \n",
    "#                    max_tokens=100, # need to be careful with this one\n",
    "#                    num_completions=1,\n",
    "#                    request_timeout=10,\n",
    "#                    verbose=False,\n",
    "#                    temperature=0.9,\n",
    "#                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
