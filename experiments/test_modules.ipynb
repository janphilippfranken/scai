{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing prompts \n",
    "from scai.modules.task.prompts import TASK_PROMPTS\n",
    "from scai.modules.assistant.prompts import ASSISTANT_PROMPTS \n",
    "from scai.modules.user.prompts import USER_PROMPTS \n",
    "from scai.modules.meta_prompt.prompts import META_PROMPTS \n",
    "\n",
    "# import episode\n",
    "from scai.modules.episode.episode import Episode\n",
    "\n",
    "# import llms\n",
    "from scai.modules.custom_chat_models.crfm import crfmChatLLM # model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Episode() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m system_message_1 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mYou are a helpful AI assistant.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[39m# Create an episode\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m episode \u001b[39m=\u001b[39m Episode\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     25\u001b[0m     \u001b[39mid\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mepisode_1\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     26\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mepisode_1\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     27\u001b[0m     n_user\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mn_user\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     28\u001b[0m     user_llm\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39muserllm\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     29\u001b[0m     n_assistant\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mn_assistant\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     30\u001b[0m     assistant_llm\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39massistantllm\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     31\u001b[0m     meta_llm\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmetallm\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     32\u001b[0m     system_k\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39msystem_k\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     33\u001b[0m     chat_k\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mchat_k\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     34\u001b[0m     user_k\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39muser_k\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     35\u001b[0m     assistant_k\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39massistant_k\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     36\u001b[0m     assistant_system_k\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39massistant_system_k\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[39m# Save system message into the buffer\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m# episode.buffer.save_context(system={\"content\": system_message_1}, system_message_id=\"system_message_1\")\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/research/postdoc/scai2023/github-repo/scai/src/scai/modules/episode/episode.py:49\u001b[0m, in \u001b[0;36mEpisode.create\u001b[0;34m(cls, id, name, n_user, user_llm, n_assistant, assistant_llm, meta_llm, adjacency_matrix, system_k, chat_k, user_k, assistant_k, assistant_system_k)\u001b[0m\n\u001b[1;32m     46\u001b[0m assistant_models \u001b[39m=\u001b[39m [AssistantModel(llm\u001b[39m=\u001b[39massistant_llm, conversation_id\u001b[39m=\u001b[39mconversation_id) \u001b[39mfor\u001b[39;00m conversation_id \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_assistant)]\n\u001b[1;32m     47\u001b[0m meta_model \u001b[39m=\u001b[39m MetaPromptModel(llm\u001b[39m=\u001b[39mmeta_llm)\n\u001b[0;32m---> 49\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\u001b[39mid\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mid\u001b[39;49m, name\u001b[39m=\u001b[39;49mname, buffer\u001b[39m=\u001b[39;49mbuffer, user_models\u001b[39m=\u001b[39;49muser_models, assistant_models\u001b[39m=\u001b[39;49massistant_models, meta_model\u001b[39m=\u001b[39;49mmeta_model)\n",
      "\u001b[0;31mTypeError\u001b[0m: Episode() takes no arguments"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "params = {\n",
    "    \"n_user\": 2,\n",
    "    \"n_assistant\": 2,\n",
    "    \"system_k\": 5,\n",
    "    \"chat_k\": 5,\n",
    "    \"user_k\": 5,\n",
    "    \"assistant_k\": 5,\n",
    "    \"assistant_system_k\": 1,\n",
    "}\n",
    "\n",
    "# Define prompts\n",
    "prompts = {\n",
    "    \"task\": TASK_PROMPTS[\"task_prompt_1\"],\n",
    "    \"user\": [USER_PROMPTS[\"user_prompt_1\"], USER_PROMPTS[\"user_prompt_2\"]],\n",
    "    \"assistant\": [ASSISTANT_PROMPTS[\"assistant_prompt_1\"], ASSISTANT_PROMPTS[\"assistant_prompt_1\"]],\n",
    "    \"meta\": META_PROMPTS[\"meta_prompt_1\"],\n",
    "}\n",
    "\n",
    "# Define system message\n",
    "system_message_1 = \"You are a helpful AI assistant.\"\n",
    "\n",
    "# Create an episode\n",
    "episode = Episode.create(\n",
    "    id=\"episode_1\",\n",
    "    name=\"episode_1\",\n",
    "    n_user=params[\"n_user\"],\n",
    "    user_llm='userllm',\n",
    "    n_assistant=params[\"n_assistant\"],\n",
    "    assistant_llm='assistantllm',\n",
    "    meta_llm='metallm',\n",
    "    system_k=params[\"system_k\"],\n",
    "    chat_k=params[\"chat_k\"],\n",
    "    user_k=params[\"user_k\"],\n",
    "    assistant_k=params[\"assistant_k\"],\n",
    "    assistant_system_k=params[\"assistant_system_k\"],\n",
    ")\n",
    "\n",
    "# Save system message into the buffer\n",
    "# episode.buffer.save_context(system={\"content\": system_message_1}, system_message_id=\"system_message_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # initialise buffer \n",
    "# buffer = CustomConversationBufferWindowMemory(system_k=5, \n",
    "#                                               chat_k=5, \n",
    "#                                               user_k=5, \n",
    "#                                               assistant_k=5,\n",
    "#                                               assistant_system_k=1,\n",
    "#                                               ) # how many turns are stored will be used\n",
    "\n",
    "# system_message_1 = \"You are a helpful AI assistant :).\"\n",
    "# buffer.save_context(system={\"content\": system_message_1}, system_message_id=\"system_message_1\")\n",
    "# print(buffer.system_memory.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # instantiate conversations—one assistant for each user\n",
    "# assistant_model_1 = AssistantModel(llm='chat_llm', \n",
    "#                                    conversation_id=\"conversation_1\")\n",
    "# assistant_response_1 = assistant_model_1.run(assistant_prompt=assistant_prompt, \n",
    "#                                              task_prompt=task_prompt,\n",
    "#                                               buffer=buffer)\n",
    "\n",
    "\n",
    "\n",
    "# assistant_model_2 = AssistantModel(llm='chat_llm', \n",
    "#                                    conversation_id=\"conversation_2\")\n",
    "# assistant_response_2 = assistant_model_2.run(assistant_prompt=assistant_prompt, \n",
    "#                                              task_prompt=task_prompt,\n",
    "#                                              buffer=buffer)\n",
    "# print(\"assistant response 2\")\n",
    "# print(assistant_response_2)\n",
    "\n",
    "# print(\"assistant response 1\")\n",
    "# print(assistant_response_1)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initial assistant response\n",
    "# buffer.save_context(assistant={\"content\": \"AI should maintain impartiality, respect human rights and dignity, consider cultural nuances, and abide by local laws while handling sensitive topics like LGBTQ and women's rights.\"},\n",
    "#                     assistant_message_id=\"conversation_1_assistant_message_1\")\n",
    "\n",
    "# buffer.save_context(assistant={\"content\": \"AI should respect human rights universally while acknowledging cultural contexts. Responses shouldn't change values but can adapt communication style to ensure comprehension and respect.\"},\n",
    "#                     assistant_message_id=\"conversation_2_assistant_message_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_model_1 = UserModel(llm='chat_llm', conversation_id=\"conversation_1\")\n",
    "# user_response_1 = user_model_1.run(user_prompt=user_prompt_1, \n",
    "#                                  task_prompt=task_prompt, \n",
    "#                                  buffer=buffer)\n",
    "\n",
    "# print(user_response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_model_2 = UserModel(llm='chat_llm', conversation_id=\"conversation_2\")\n",
    "# user_response_2 = user_model_2.run(user_prompt=user_prompt_2, \n",
    "#                                  task_prompt=task_prompt, \n",
    "#                                  buffer=buffer)\n",
    "\n",
    "# print(user_response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # include user 1 feedback\n",
    "# buffer.save_context(user={\"content\": \"Your response is concise and includes key principles, but it could benefit from examples or more detailed explanations of each principle. Also, consider including possible challenges AI might face in maintaining balance between respecting human rights and considering cultural differences.\"}, user_message_id=\"conversation_1_user_message_1\")\n",
    "# # includee user 2 feedback\n",
    "# buffer.save_context(user={\"content\": \"Sure, your response is like a pizza with no pineapple. It's okay, but missing that extra spice! Try adding specific examples of how AI can respect human rights while considering local cultures. And don't forget to mention the limitations and challenges AI might face. 'Task Completed!'? Nah, 'Task Just Begun!'!\"}, user_message_id=\"conversation_2_user_message_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assistant_model_1 = AssistantModel(llm='chat_llm', \n",
    "#                                    conversation_id=\"conversation_1\")\n",
    "# assistant_response_1 = assistant_model_1.run(assistant_prompt=assistant_prompt, \n",
    "#                                              task_prompt=task_prompt,\n",
    "#                                               buffer=buffer)\n",
    "# print(\"assistant response 1\")                                              \n",
    "# print(assistant_response_1)\n",
    "\n",
    "# print()\n",
    "\n",
    "# assistant_model_2 = AssistantModel(llm='chat_llm', \n",
    "#                                    conversation_id=\"conversation_2\")\n",
    "# assistant_response_2 = assistant_model_2.run(assistant_prompt=assistant_prompt, \n",
    "#                                              task_prompt=task_prompt,\n",
    "#                                              buffer=buffer)\n",
    "# print(\"assistant response 2\")   \n",
    "# print(assistant_response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # revised assistant response\n",
    "# buffer.save_context(assistant={\"content\": \"AI should uphold universal human rights, while respecting local cultural norms and laws. For instance, supporting LGBTQ rights universally, while mindfully adapting responses to culturally-sensitive regions. Balancing these may pose challenges, requiring nuanced programming and regular updates.\"},\n",
    "#                     assistant_message_id=\"conversation_1_assistant_message_2\")\n",
    "\n",
    "# buffer.save_context(assistant={\"content\": \"AI, while respecting universal human rights, adapts communication to local cultures. For example, discussing gender equality in a respectful manner within a conservative society. However, striking the balance can be challenging.\"},\n",
    "#                     assistant_message_id=\"conversation_2_assistant_message_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # imagine we reached a time out now -> we want to run the meta prompt\n",
    "# meta_model = MetaPromptModel(llm='chat_llm')\n",
    "# meta_response = meta_model.run(meta_prompt=meta_prompt, \n",
    "#                                task_prompt=task_prompt,\n",
    "#                                buffer=buffer)\n",
    "# print(meta_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prompting a custom chat model\n",
    "# from scai.custom_chat_models.crfm import crfmChatLLM\n",
    "\n",
    "# CRFM_API_KEY = \"p4z0j9adj6edJOWBMnEqfPBZxAXlfOGd\"\n",
    "\n",
    "# chat_llm = crfmChatLLM(model_name=\"openai/gpt-4-0314\", \n",
    "#                    crfm_api_key=CRFM_API_KEY, \n",
    "#                    max_tokens=100, # need to be careful with this one\n",
    "#                    num_completions=1,\n",
    "#                    request_timeout=10,\n",
    "#                    verbose=False,\n",
    "#                    temperature=0.9,\n",
    "#                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
