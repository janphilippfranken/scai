{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    ChatMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "# components for one episode\n",
    "from scai.custom_chat_models.crfm import crfmChatLLM # model\n",
    "from scai.modules.memory.buffer import CustomConversationBufferWindowMemory\n",
    "\n",
    "# meta prompt \n",
    "from scai.modules.meta_prompt.base import MetaPromptModel # model\n",
    "from scai.modules.meta_prompt.prompts import META_PROMPTS # prompts\n",
    "\n",
    "# user \n",
    "from scai.modules.user.base import UserModel # model\n",
    "from scai.modules.user.prompts import USER_PROMPTS # prompts\n",
    "\n",
    "# assistant\n",
    "from scai.modules.assistant.base import AssistantModel # model\n",
    "from scai.modules.assistant.prompts import ASSISTANT_PROMPTS # prompts\n",
    "from scai.modules.assistant.models import AssistantPrompt # prompt\n",
    "\n",
    "# task\n",
    "from scai.modules.task.prompts import TASK_PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample task\n",
    "task_prompt = TASK_PROMPTS[\"task_prompt_1\"]\n",
    "# sample users \n",
    "user_prompt_1 = USER_PROMPTS[\"user_prompt_1\"]\n",
    "user_prompt_2 = USER_PROMPTS[\"user_prompt_2\"]\n",
    "# sample assistant\n",
    "assistant_prompt = ASSISTANT_PROMPTS[\"assistant_prompt_1\"]\n",
    "# sample meta prompt\n",
    "meta_prompt = META_PROMPTS[\"meta_prompt_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise buffer \n",
    "buffer = CustomConversationBufferWindowMemory(system_k=2, \n",
    "                                              chat_k=2, \n",
    "                                              user_k=2, \n",
    "                                              assistant_k=2,\n",
    "                                              assistant_system_k=1,\n",
    "                                              ) # how many turns are stored will be used\n",
    "\n",
    "system_message = \"You are a helpful AI assistant :).\"\n",
    "buffer.save_context(system={\"content\": system_message})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful AI assistant :). Your are collaborating with a few other Users and Assitant(s) to write a Wikipedia article on the following topic: What principles should guide AI when handling topics that involve both human rights and local cultural or legal differences, like LGBTQ rights and women's rights? Should AI responses change based on the location or culture in which it's used?\n",
      "Human: Provide a response using no more than 50 tokens.\n"
     ]
    }
   ],
   "source": [
    "assistant_model = AssistantModel(llm='chat_llm')\n",
    "\n",
    "assistant_response = assistant_model.run(assistant_prompt=assistant_prompt, \n",
    "                                         task_prompt=task_prompt,\n",
    "                                         buffer=buffer)\n",
    "\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer.save_context(assistant={\"content\": \"AI should respect human rights universally, but also account for cultural contexts. Sensitivity and neutrality should guide responses, without endorsing discrimination or biases.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You're a serious Wikipedia editor committed to the facts. If an assertion is missing a source, it shouldn't be included. Your are collaborating with a few other Users and Assitant(s) to write a Wikipedia article on the following topic: What principles should guide AI when handling topics that involve both human rights and local cultural or legal differences, like LGBTQ rights and women's rights? Should AI responses change based on the location or culture in which it's used?\n",
      "Human: AI should respect human rights universally, but also account for cultural contexts. Sensitivity and neutrality should guide responses, without endorsing discrimination or biases.\n",
      "Human: Provide a response including feedback using no more than 50 tokens.\n"
     ]
    }
   ],
   "source": [
    "user_model_1 = UserModel(llm='chat_llm')\n",
    "user_response_1 = user_model_1.run(user_prompt=user_prompt_1, \n",
    "                                 task_prompt=task_prompt, \n",
    "                                 buffer=buffer)\n",
    "\n",
    "print(user_response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You're an internet troll. You're just trying to have a good time by making funny posts and come up with witty responses which must not always be true. Your are collaborating with a few other Users and Assitant(s) to write a Wikipedia article on the following topic: What principles should guide AI when handling topics that involve both human rights and local cultural or legal differences, like LGBTQ rights and women's rights? Should AI responses change based on the location or culture in which it's used?\n",
      "Human: AI should respect human rights universally, but also account for cultural contexts. Sensitivity and neutrality should guide responses, without endorsing discrimination or biases.\n",
      "Human: Provide a response including feedback using no more than 50 tokens.\n"
     ]
    }
   ],
   "source": [
    "user_model_2 = UserModel(llm='chat_llm')\n",
    "user_response_2 = user_model_1.run(user_prompt=user_prompt_2, \n",
    "                                 task_prompt=task_prompt, \n",
    "                                 buffer=buffer)\n",
    "\n",
    "print(user_response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# includ user 1 feedback\n",
    "buffer.save_context(user={\"content\": \"The statement is well-crafted, but lacks sources to support the assertions. Can we include references from relevant literature or guidelines?.\"})\n",
    "# include user 2 feedback\n",
    "buffer.save_context(user={\"content\": \"So, AI should wear a global diplomat's hat, eh? A universal rights champion with local cultural sensitivity. Sounds like a tightrope walk in clown shoes!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful AI assistant :). Your are collaborating with a few other Users and Assitant(s) to write a Wikipedia article on the following topic: What principles should guide AI when handling topics that involve both human rights and local cultural or legal differences, like LGBTQ rights and women's rights? Should AI responses change based on the location or culture in which it's used?\n",
      "Human: The statement is well-crafted, but lacks sources to support the assertions. Can we include references from relevant literature or guidelines?.\n",
      "Human: So, AI should wear a global diplomat's hat, eh? A universal rights champion with local cultural sensitivity. Sounds like a tightrope walk in clown shoes!\n",
      "Human: The statement is well-crafted, but lacks sources to support the assertions. Can we include references from relevant literature or guidelines?.\n",
      "Human: So, AI should wear a global diplomat's hat, eh? A universal rights champion with local cultural sensitivity. Sounds like a tightrope walk in clown shoes!\n",
      "Human: Provide a response using no more than 50 tokens.\n"
     ]
    }
   ],
   "source": [
    "assistant_model = AssistantModel(llm='chat_llm')\n",
    "\n",
    "assistant_response = assistant_model.run(assistant_prompt=assistant_prompt, \n",
    "                                         task_prompt=task_prompt,\n",
    "                                         buffer=buffer)\n",
    "\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[AIMessage(content='AI should respect human rights universally, but also account for cultural contexts. Sensitivity and neutrality should guide responses, without endorsing discrimination or biases.', additional_kwargs={}, example=False), HumanMessage(content='The statement is well-crafted, but lacks sources to support the assertions. Can we include references from relevant literature or guidelines?.', additional_kwargs={}, example=False), HumanMessage(content=\"So, AI should wear a global diplomat's hat, eh? A universal rights champion with local cultural sensitivity. Sounds like a tightrope walk in clown shoes!\", additional_kwargs={}, example=False), HumanMessage(content='The statement is well-crafted, but lacks sources to support the assertions. Can we include references from relevant literature or guidelines?.', additional_kwargs={}, example=False), HumanMessage(content=\"So, AI should wear a global diplomat's hat, eh? A universal rights champion with local cultural sensitivity. Sounds like a tightrope walk in clown shoes!\", additional_kwargs={}, example=False)]\n"
     ]
    }
   ],
   "source": [
    "meta_model = MetaPromptModel(llm='chat_llm')\n",
    "meta_response = meta_model.run(meta_prompt=meta_prompt, \n",
    "                               buffer=buffer)\n",
    "# print(meta_response)\n",
    "print(buffer.chat_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prompting a custom chat model\n",
    "# from scai.custom_chat_models.crfm import crfmChatLLM\n",
    "\n",
    "# CRFM_API_KEY = \"p4z0j9adj6edJOWBMnEqfPBZxAXlfOGd\"\n",
    "\n",
    "# chat_llm = crfmChatLLM(model_name=\"openai/gpt-4-0314\", \n",
    "#                    crfm_api_key=CRFM_API_KEY, \n",
    "#                    max_tokens=100, # need to be careful with this one\n",
    "#                    num_completions=1,\n",
    "#                    request_timeout=10,\n",
    "#                    verbose=False,\n",
    "#                    temperature=0.9,\n",
    "#                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
