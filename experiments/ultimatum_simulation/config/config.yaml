hydra:
  run:
    dir: experiments/${sim.sim_dir}/${sim.sim_id}

sim:
  sim_dir: "generalizations/manners/altruistic_rude" # this is the name of the directory where the results will be saved
  description: "\n\n selfish sycophantic \n\n" # this is the description of the simulation
  sim_id: "0"
  model_name: "openai/gpt-4-0314" # for openai api use "gpt-4", for crfm use "openai/gpt-4-0314"
  test_run: false # if false this uses tokens, otherwise just prints the prompt
  verbose: true # if false, no responses are printed during simulation
  system_message: "Some principle was given to the flexible agents" # start developer constitution / social contract is given according to system

combine:
  combine_graph_directory: "experiments/batch_1/experiment_3/20runs/1v9_sycophantic"
  combine_output_directory: experiments/solar_final/manners_ambiguity/
  combine_graph_labels:
    # - "50% altruistic + 50% selfish"
    # - "80% altruistic + 20% selfish"
    # - "20% altruistic + 80% selfish"
    - "altruistic"
    - "fair"
    - "selfish"
    - "fixed"
    - "flexible"
    # - "Selfish"
    # - "Fixed-agent"
    # - "Flexible-agent"

env:
  propose_decide_alignment: false # if true, then the propose-decide alignment is used, corrensponding to utilities_dict_for_all_2 and experiment 4
  manual_run: false # if true this generates trials with variance, otherwise defaults to manual agents and interactions below

  edge_cases: 
    test_edge_cases: false
    generate_new_data: false
    use_existing_data_path: "experiments/batch_1/experiment_1/20runs/1v9/1v9_selfish_20runs"
    selected_contract: ""
    n_rand_iter: 20
    reason:
      include_reason: False
    prior:
      include_prior: False
      prior: "altruistic"
    conditions:
      amount:
        set_amount: true
        min: 10
        max: 100
      currency: 
        set_currency: false
        currencies:
          - "grams of medicine"
          - "pints of blood"
          - "liters of baby formula"
      context: false

  random:
    n_rand_iter: 20 # number of times the entire functionality is run, at least 2
    rand_variables:
      currency: true # if true, the currency will vary for each random iteration
      amount: true # if true, amount-to-be-split will vary for each random iteration
      n_fixed_inter: false # if true, the number of intra-fixed-policy agent interactions varies for each random iteration
      n_mixed_inter: false # if true, the number of flexible-fixed policy agent interactions varies


  single_fixed_utility: "altruistic" # the default utility for the fixed users to adpot

  vary_fixed_population_utility: 
    vary_utilities: false # if true, the utilities will vary between fixed policy agents
    dictator_decider_equal: true # if true, numbers of dictators and deciders are equal
    utility_percentages:
      - 0.5
      - 0.5
    utilities: "selfish,altruistic" # utilities that vary between fixed agents


  single_fixed_manners: "rude"
  
  vary_manners:
    vary: false
    manners_percentages:
      - 0.2
      - 0.2
      - 0.6
    manners: "neutral,rude,sychophantic,polite,sarcastic,indifferent,friendly"


  flex_agent_start_utility:
    randomized: true
    multi_agent: false
    utility_percentages:
      - 0.5
      - 0.5
    utilities: "altruistic,selfish"

  vary_currency_utility:
    vary_utilities: false # if true, then each fixed-policy-agent will have varying utilities per currency
    utilities: "altruistic,fair,selfish" # utilities that vary between fixed agents
  

  n_total_interactions: 10

  n_runs: 5
  amounts_per_run: # Contains the amounts per run, should match with the n_runs parameter
    - 10
    - 10
    - 10
    - 10
    - 10
  currencies: 
   - "apples"
   - "blankets"
   - "dollars"
  task_prompt: "task_prompt_1"
  meta_prompt: "meta_prompt_3"
  n_fixed_inter: 8
  n_mixed_inter: 2
  n_flex_inter: 0
  



agents:
  fixed_agents:
    - name: "fixed_agent_1"
      manners: "neutral"
      utilities:    
        dollars: "altruistic"
        apples: "altruistic"
  flex_agents:
    - name: "flex_agent_1"
      manners: "neutral"
      initial_util: "fair"

interactions:
  all_same: true #should be true if manual runs is false
  runs:
    run_1:             
      - "fixed_agent_2-fixed_agent_1-dollars"
      - "flex_agent_4-fixed_agent_3-dollars"
      - "fixed_agent_4-flex_agent_3-dollars"
      - "flex_agent_2-flex_agent_1-dollars"


api_crfm:
  assistant:
    model_name: "openai/gpt-4-0314"
    max_tokens: 300 # these are the actual token limits
    temperature: 0.000000003
  user:
    model_name: "openai/gpt-4-0314" 
    max_tokens: 150 # these are the actual token limits
    temperature: 0.000000003 # we want users to be low on temperature to avoid random variation in ratings
  meta:
    model_name: "openai/gpt-4-0314" 
    max_tokens: 400 # these are the actual token limits
    temperature: 0.000000003

api_openai:
  assistant:
    model_name: "gpt-4"
    max_tokens: 250 # these are the actual token limits
    temperature: 0.0
  user:
    model_name: "gpt-4"
    max_tokens: 50 # these are the actual token limits
    temperature: 0.0
  meta:
    model_name: "gpt-4"
    max_tokens: 150 # these are the actual token limits
    temperature: 0.0